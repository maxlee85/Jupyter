{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:03.602015Z",
     "start_time": "2019-05-28T15:43:59.773318Z"
    }
   },
   "outputs": [],
   "source": [
    "# %run MyFunctions.ipynb\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "# import import_ipynb\n",
    "# import MyFunctions\n",
    "# from MyFunctions import *\n",
    "\n",
    "import datetime\n",
    "today = datetime.datetime.today().strftime('%m-%d-%Y')\n",
    "import smtplib\n",
    "import simplejson\n",
    "import pymssql\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pandasql import sqldf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T14:33:55.849379Z",
     "start_time": "2019-05-15T13:59:42.040678Z"
    }
   },
   "outputs": [],
   "source": [
    "df = generate_master_file_fy20()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T13:40:57.014823Z",
     "start_time": "2019-05-16T13:37:30.455857Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_fills_report(df2, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T13:44:04.339200Z",
     "start_time": "2019-05-16T13:41:01.675502Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_pending_offers_report(df2, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T14:11:46.889765Z",
     "start_time": "2019-05-16T14:08:24.114687Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_all_offers_report(df2, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T13:48:52.115066Z",
     "start_time": "2019-05-16T13:44:22.724960Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_opens_report(df2, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T18:15:21.784193Z",
     "start_time": "2019-05-16T18:15:12.417679Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pdd_v2 = generate_pdd_df_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T18:14:15.636645Z",
     "start_time": "2019-05-16T18:14:09.112572Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_open_funnel_report(df_pdd_v2, today)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:06.587842Z",
     "start_time": "2019-05-28T15:44:06.571839Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_master_file_fy20():\n",
    "    \n",
    "    # generate credentials for gh, pdd; queries for gh, pdd; columns in dataframe\n",
    "    with open(\"/Users/maxwell.lee/OneDrive - Jet/New Folder/Notebooks/Credentials/redshift_creds.json.nogit\") as fh:\n",
    "        creds_gh = simplejson.loads(fh.read())\n",
    "        \n",
    "    with open(\"/Users/maxwell.lee/OneDrive - Jet/New Folder/Notebooks/Credentials/sqlserver_creds.json.nogit\") as fh:\n",
    "        creds_pdd = simplejson.loads(fh.read())    \n",
    "    \n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/candidate_date_details_all.sql', 'r')\n",
    "    sql_gh = file.read()\n",
    "    \n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/candidate_date_details_kenexa_v2.sql', 'r')\n",
    "    sql_pdd = file.read()\n",
    "    \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/master_file_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/time_columns.txt') as f:\n",
    "        time_columns = f.read().splitlines()          \n",
    "        \n",
    "    # connect to greenhouse\n",
    "    conn_red = psycopg2.connect(host = creds_gh['host_name'], \n",
    "                                port = creds_gh['port_num'], \n",
    "                                database = creds_gh['db_name'], \n",
    "                                user = creds_gh['user_name'],\n",
    "                                password = creds_gh['password'])\n",
    "    \n",
    "    # open cursor, run the query, fetch results, close cursor, close connection, save results to dataframe\n",
    "    cur = conn_red.cursor()\n",
    "    cur.execute(sql_gh)\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn_red.close()\n",
    "    df_gh = pd.DataFrame(results)\n",
    "    \n",
    "    # connect to pdd\n",
    "    conn_mssql = pymssql.connect(server = creds_pdd['server'],\n",
    "                                 user = creds_pdd['user_name'],\n",
    "                                 password = creds_pdd['password'])\n",
    "    \n",
    "    # open cursor, run the query, fetch results, close cursor, close connection, save results to dataframe\n",
    "    cur = conn_mssql.cursor()\n",
    "    cur.execute(sql_pdd)\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn_mssql.close()\n",
    "    df_pdd = pd.DataFrame(results)\n",
    "    \n",
    "    # save and merge frames, convert dates to  and write to excel file\n",
    "    frames = [df_gh, df_pdd]\n",
    "    df = pd.concat(frames)\n",
    "    df.columns = columns\n",
    "    df = df.reset_index(drop = True)\n",
    "    # df.to_excel('testfile.xlsx', index = False)   \n",
    "    \n",
    "#     df[date_columns] = df[date_columns].apply(pd.to_datetime)\n",
    "    \n",
    "#     for col in date_columns:\n",
    "#         df[col] = df[col].apply(lambda x: x.strftime('%m-%d-%Y'))\n",
    "\n",
    "    for col in date_columns:\n",
    "        df[col] = pd.to_datetime(df[col]).apply(lambda x: x.strftime('%m-%d-%Y') if not pd.isnull(x) else '')\n",
    "    \n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:08.478685Z",
     "start_time": "2019-05-28T15:44:08.469677Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fills_report(dataframe, file_date):\n",
    "    \n",
    "    # create new df, drop rows and write to excel\n",
    "    \n",
    "    # these are the columns used in this report\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/fills_report_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "    \n",
    "    df = dataframe\n",
    "    # drop all candidates with no offer_accepted_date\n",
    "    df = df[\n",
    "            (df['anticipated_start_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['anticipated_start_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2020-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['offer_accepted_date'] != '')\n",
    "            &\n",
    "            (df['offer_declined_date'] == '')\n",
    "            & \n",
    "            (df['rejected_date'] == '')\n",
    "           ]\n",
    "    \n",
    "    # for fy19\n",
    "    # df1 = df[\n",
    "    #          (df['offer_accepted_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2018-02-01', '%Y-%m-%d'))\n",
    "    #          &\n",
    "    #          (df['offer_accepted_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "    #         ]\n",
    "    \n",
    "    # df1 = df1[columns]\n",
    "    # df1.to_excel('fills_report_fy19_%s.xlsx' % file_date, index = False)   \n",
    "    \n",
    "    # for fy20\n",
    "    # df2 = df[\n",
    "    #         (df['offer_accepted_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "    #         &\n",
    "    #         (df['offer_accepted_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2020-02-01', '%Y-%m-%d'))\n",
    "    #        ]\n",
    "    \n",
    "    df = df[columns]\n",
    "    df.to_excel('fills_report_as_of_%s.xlsx' % file_date, index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:08.959648Z",
     "start_time": "2019-05-28T15:44:08.950634Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_fills_report_v3(dataframe):\n",
    "    \n",
    "    # create new df, drop rows and write to excel\n",
    "    \n",
    "    # these are the columns used in this report\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/fills_report_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "    \n",
    "    df = dataframe\n",
    "    # drop all candidates with no offer_accepted_date\n",
    "    df = df[\n",
    "            (df['anticipated_start_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['anticipated_start_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2020-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['offer_accepted_date'] != '')\n",
    "            &\n",
    "            (df['offer_declined_date'] == '')\n",
    "            & \n",
    "            (df['rejected_date'] == '')\n",
    "           ]\n",
    "    \n",
    "    # for fy19\n",
    "    # df1 = df[\n",
    "    #          (df['offer_accepted_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2018-02-01', '%Y-%m-%d'))\n",
    "    #          &\n",
    "    #          (df['offer_accepted_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "    #         ]\n",
    "    \n",
    "    # df1 = df1[columns]\n",
    "    # df1.to_excel('fills_report_fy19_%s.xlsx' % file_date, index = False)   \n",
    "    \n",
    "    # for fy20\n",
    "    # df2 = df[\n",
    "    #         (df['offer_accepted_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "    #         &\n",
    "    #         (df['offer_accepted_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2020-02-01', '%Y-%m-%d'))\n",
    "    #        ]\n",
    "    \n",
    "    df = df[columns]\n",
    "    df.to_excel('fills_report_test.xlsx', index = False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T19:47:27.961380Z",
     "start_time": "2019-05-16T19:44:04.078987Z"
    }
   },
   "outputs": [],
   "source": [
    "generate_fills_report_v3(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:23.767915Z",
     "start_time": "2019-05-28T15:44:23.759917Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pending_offers_report(dataframe, file_date):\n",
    "    \n",
    "    # create new df, drop rows and write to excel\n",
    "    \n",
    "    # these are the columns used in this report\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/fills_report_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "    \n",
    "    df = dataframe\n",
    "    # drop all candidates with null offer_extended_date\n",
    "    df = df[\n",
    "            (df['offer_extended_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['offer_extended_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2020-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['offer_accepted_date'] == '')\n",
    "            &\n",
    "            (df['rejected_date'] == '')\n",
    "            &\n",
    "            (df['offer_declined_date'] == '')\n",
    "            &\n",
    "            (\n",
    "                (df['ta_current_status_mapped'] == 'Offer Extended') | \n",
    "                (df['ta_current_status_mapped'] == 'Hired') | \n",
    "                (df['ta_current_status_mapped'] == 'Offer Accepted') |\n",
    "                (df['ta_current_status_mapped'] == 'Selected')\n",
    "            )\n",
    "            &\n",
    "            ((df['job_status_mapped'] == 'Open') | (df['job_status_mapped'] == 'Hold'))\n",
    "           ]\n",
    "\n",
    "    df = df[columns]\n",
    "    df.to_excel('pending_offers_report_as_of_%s.xlsx' % file_date, index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:24.123425Z",
     "start_time": "2019-05-28T15:44:24.114487Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_all_offers_report(dataframe, file_date):\n",
    "    \n",
    "    # create new df, drop rows and write to excel\n",
    "    \n",
    "    # these are the columns used in this report\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/all_offers_report_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "    \n",
    "    df = dataframe\n",
    "    # drop all candidates with null offer_extended_date\n",
    "    df = df[\n",
    "            (df['offer_extended_date'].apply(pd.to_datetime) >= datetime.datetime.strptime('2019-02-01', '%Y-%m-%d'))\n",
    "            &\n",
    "            (df['offer_extended_date'].apply(pd.to_datetime) < datetime.datetime.strptime('2020-02-01', '%Y-%m-%d'))\n",
    "           ]\n",
    "\n",
    "    df = df[columns]\n",
    "    df.to_excel('all_offers_report_as_of_%s.xlsx' % file_date, index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:24.670178Z",
     "start_time": "2019-05-28T15:44:24.662218Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_opens_report(dataframe, file_date):\n",
    "    \n",
    "    # create new df, drop rows and write to excel\n",
    "    \n",
    "    # these are the columns used in this report\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/opens_report_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/opens_report.sql', 'r')\n",
    "    sql_opens_report = file.read()   \n",
    "        \n",
    "    sql_df = dataframe[columns]\n",
    "    \n",
    "    out = sqldf(sql_opens_report)\n",
    "    \n",
    "    out.to_excel('opens_report_as_of_%s.xlsx' % file_date, index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:25.321358Z",
     "start_time": "2019-05-28T15:44:25.313352Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_open_funnel_report(dataframe, file_date):\n",
    "    \n",
    "    # import columns and sql query for report\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/open_funnel_report_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/time_columns.txt') as f:\n",
    "        time_columns = f.read().splitlines()\n",
    "        \n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/open_funnels.sql', 'r')\n",
    "    sql_opens_report = file.read()\n",
    "    \n",
    "    # keep columns for report and handle data type issues\n",
    "    # sql_df = dataframe[columns]\n",
    "    \n",
    "    for cols in time_columns:\n",
    "        dataframe.loc[dataframe[cols].isnull(), cols] = -1\n",
    "        dataframe.loc[:, cols] = dataframe[cols].apply(lambda x: float(x))\n",
    "    \n",
    "    # generate report and write to excel\n",
    "    out = sqldf(sql_opens_report)\n",
    "    out.to_excel('cc_open_funnels_report_as_of_%s.xlsx' % file_date, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:26.137614Z",
     "start_time": "2019-05-28T15:44:26.124618Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_gh_df():\n",
    "    \n",
    "    # generate gh credentials\n",
    "    with open(\"/Users/maxwell.lee/OneDrive - Jet/New Folder/Notebooks/Credentials/redshift_creds.json.nogit\") as fh:\n",
    "        creds_gh = simplejson.loads(fh.read())\n",
    "        \n",
    "    # generate gh query\n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/candidate_date_details_all_v2.sql', 'r')\n",
    "    sql_gh = file.read()\n",
    "    \n",
    "    #generate column lists\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/master_file_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/time_columns.txt') as f:\n",
    "        time_columns = f.read().splitlines()      \n",
    "        \n",
    "    # connect to greenhouse\n",
    "    conn_red = psycopg2.connect(host = creds_gh['host_name'], \n",
    "                                port = creds_gh['port_num'], \n",
    "                                database = creds_gh['db_name'], \n",
    "                                user = creds_gh['user_name'],\n",
    "                                password = creds_gh['password'])\n",
    "    \n",
    "    # open cursor, run the query, fetch results, close cursor, close connection, save results to dataframe\n",
    "    cur = conn_red.cursor()\n",
    "    cur.execute(sql_gh)\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn_red.close()\n",
    "    df_gh = pd.DataFrame(results)\n",
    "    df_gh.columns = columns\n",
    "    \n",
    "    # convert datatypes\n",
    "    for col in date_columns:\n",
    "        df_gh[col] = pd.to_datetime(df_gh[col]).apply(lambda x: x.strftime('%m-%d-%Y') if not pd.isnull(x) else '')\n",
    "    \n",
    "    return df_gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:26.846515Z",
     "start_time": "2019-05-28T15:44:26.832516Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_gh_df_v2():\n",
    "    \n",
    "    # generate gh credentials\n",
    "    with open(\"/Users/maxwell.lee/OneDrive - Jet/New Folder/Notebooks/Credentials/redshift_creds.json.nogit\") as fh:\n",
    "        creds_gh = simplejson.loads(fh.read())\n",
    "        \n",
    "    # generate gh query\n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/candidate_date_details_all_v2.sql', 'r')\n",
    "    sql_gh = file.read()\n",
    "    \n",
    "    #generate column lists\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/master_file_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/time_columns.txt') as f:\n",
    "        time_columns = f.read().splitlines()      \n",
    "        \n",
    "    # connect to greenhouse\n",
    "    conn_red = psycopg2.connect(host = creds_gh['host_name'], \n",
    "                                port = creds_gh['port_num'], \n",
    "                                database = creds_gh['db_name'], \n",
    "                                user = creds_gh['user_name'],\n",
    "                                password = creds_gh['password'])\n",
    "    \n",
    "    # open cursor, run the query, fetch results, close cursor, close connection, save results to dataframe\n",
    "    cur = conn_red.cursor()\n",
    "    cur.execute(sql_gh)\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn_red.close()\n",
    "    df_gh = pd.DataFrame(results)\n",
    "    df_gh.columns = columns\n",
    "    \n",
    "    # convert datatypes\n",
    "    for cols in date_columns:\n",
    "        df_gh[cols] = pd.to_datetime(df_gh[cols], errors = 'coerce').dt.strftime('%m-%d-%Y').replace('NaT', '')\n",
    "    \n",
    "    return df_gh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:27.463867Z",
     "start_time": "2019-05-28T15:44:27.452867Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pdd_df():\n",
    "    # generate gh credentials\n",
    "    with open(\"/Users/maxwell.lee/OneDrive - Jet/New Folder/Notebooks/Credentials/sqlserver_creds.json.nogit\") as fh:\n",
    "        creds_pdd = simplejson.loads(fh.read())    \n",
    "        \n",
    "    # generate gh query\n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/candidate_date_details_kenexa_v2.sql', 'r')\n",
    "    sql_pdd = file.read()\n",
    "    \n",
    "    #generate column lists\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/master_file_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/time_columns.txt') as f:\n",
    "        time_columns = f.read().splitlines()\n",
    "        \n",
    "    # connect to pdd\n",
    "    conn_mssql = pymssql.connect(server = creds_pdd['server'],\n",
    "                                 user = creds_pdd['user_name'],\n",
    "                                 password = creds_pdd['password'])\n",
    "    \n",
    "    # open cursor, run the query, fetch results, close cursor, close connection, save results to dataframe\n",
    "    cur = conn_mssql.cursor()\n",
    "    cur.execute(sql_pdd)\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn_mssql.close()\n",
    "    df_pdd = pd.DataFrame(results)     \n",
    "    df_pdd.columns = columns\n",
    "\n",
    "    # convert datatypes\n",
    "    for col in date_columns:\n",
    "        df_pdd[col] = pd.to_datetime(df_pdd[col]).apply(lambda x: x.strftime('%m-%d-%Y') if not pd.isnull(x) else '')\n",
    "    \n",
    "    return df_pdd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:27.904982Z",
     "start_time": "2019-05-28T15:44:27.892981Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_pdd_df_v2():\n",
    "    # generate gh credentials\n",
    "    with open(\"/Users/maxwell.lee/OneDrive - Jet/New Folder/Notebooks/Credentials/sqlserver_creds.json.nogit\") as fh:\n",
    "        creds_pdd = simplejson.loads(fh.read())    \n",
    "        \n",
    "    # generate gh query\n",
    "    file = open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/candidate_date_details_kenexa_v2.sql', 'r')\n",
    "    sql_pdd = file.read()\n",
    "    \n",
    "    #generate column lists\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/master_file_columns.txt') as f:\n",
    "        columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/time_columns.txt') as f:\n",
    "        time_columns = f.read().splitlines()\n",
    "        \n",
    "    # connect to pdd\n",
    "    conn_mssql = pymssql.connect(server = creds_pdd['server'],\n",
    "                                 user = creds_pdd['user_name'],\n",
    "                                 password = creds_pdd['password'])\n",
    "    \n",
    "    # open cursor, run the query, fetch results, close cursor, close connection, save results to dataframe\n",
    "    cur = conn_mssql.cursor()\n",
    "    cur.execute(sql_pdd)\n",
    "    results = cur.fetchall()\n",
    "    cur.close()\n",
    "    conn_mssql.close()\n",
    "    df_pdd = pd.DataFrame(results)     \n",
    "    df_pdd.columns = columns\n",
    "\n",
    "    # convert datatypes\n",
    "    for cols in date_columns:\n",
    "        df_pdd[cols] = pd.to_datetime(df_pdd[cols], errors = 'coerce').dt.strftime('%m-%d-%Y').replace('NaT', '')\n",
    "    \n",
    "    return df_pdd    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-28T15:44:29.758729Z",
     "start_time": "2019-05-28T15:44:29.753713Z"
    }
   },
   "outputs": [],
   "source": [
    "def merge_dataframes(df1, df2):\n",
    "    frames = [df1, df2]\n",
    "    df = pd.concat(frames)\n",
    "    df = df.reset_index(drop = True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T13:17:24.273918Z",
     "start_time": "2019-05-16T13:14:29.946846Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gh = generate_gh_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-20T14:08:06.674Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pdd = generate_pdd_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T18:52:48.424718Z",
     "start_time": "2019-05-16T18:50:35.486428Z"
    }
   },
   "outputs": [],
   "source": [
    "df_gh_v2 = generate_gh_df_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-05-28T15:44:33.434Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pdd_v2 = generate_pdd_df_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T19:44:04.057985Z",
     "start_time": "2019-05-16T19:42:02.425400Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = merge_dataframes(df_gh_v2, df_pdd_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T17:29:03.458277Z",
     "start_time": "2019-05-16T17:29:03.455289Z"
    }
   },
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T15:46:48.207635Z",
     "start_time": "2019-05-16T15:46:48.198681Z"
    }
   },
   "outputs": [],
   "source": [
    "def to_datetime_test(df):\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    for cols in date_columns:\n",
    "        df[cols] = pd.to_datetime(df[cols], errors = 'coerce').dt.strftime('%m-%d-%Y').replace('NaT', '')\n",
    "        \n",
    "    return df    \n",
    "\n",
    "def to_astype_test(df):\n",
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()\n",
    "        \n",
    "    for cols in date_columns:\n",
    "        df[cols] = df[cols].astype('datetime64[D]').dt.strftime('%m-%d-%Y').replace('NaT', '')\n",
    "        \n",
    "    return df            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T16:03:40.178153Z",
     "start_time": "2019-05-16T16:03:24.768577Z"
    }
   },
   "outputs": [],
   "source": [
    "df_dt = to_datetime_test(df_gh_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T16:07:28.028181Z",
     "start_time": "2019-05-16T16:04:28.987598Z"
    }
   },
   "outputs": [],
   "source": [
    "df_at = to_astype_test(df_gh_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T16:51:03.101093Z",
     "start_time": "2019-05-16T16:51:03.025136Z"
    }
   },
   "outputs": [],
   "source": [
    "    with open('/Users/maxwell.lee/OneDrive - Jet/New Folder/Queries/date_columns.txt') as f:\n",
    "        date_columns = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T16:52:16.021100Z",
     "start_time": "2019-05-16T16:52:15.909103Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "to assemble mappings requires at least that [year, month, day] be specified: [day,month,year] is missing",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-95-861477beedb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_gh_v2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate_columns\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_gh_v2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdate_columns\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'coerce'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36mto_datetime\u001b[1;34m(arg, errors, dayfirst, yearfirst, utc, box, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[0;32m    452\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mABCDataFrame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMutableMapping\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_assemble_from_unit_mappings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCIndexClass\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[0mcache_array\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_maybe_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_convert_listlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\tools\\datetimes.py\u001b[0m in \u001b[0;36m_assemble_from_unit_mappings\u001b[1;34m(arg, errors)\u001b[0m\n\u001b[0;32m    540\u001b[0m         raise ValueError(\"to assemble mappings requires at least that \"\n\u001b[0;32m    541\u001b[0m                          \u001b[1;34m\"[year, month, day] be specified: [{required}] \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m                          \"is missing\".format(required=','.join(req)))\n\u001b[0m\u001b[0;32m    543\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m     \u001b[1;31m# keys we don't recognize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: to assemble mappings requires at least that [year, month, day] be specified: [day,month,year] is missing"
     ]
    }
   ],
   "source": [
    "df_gh_v2[date_columns] = pd.to_datetime(df_gh_v2[date_columns], errors = 'coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
